{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bita08b587d483f407db985a7a8847d4fb9",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Assignemnt 1 - Fisher's Exact Test\n",
    "\n",
    "### Null hypothesis:\n",
    "Samples **A** and **B** were taken from the same probability distribution and the differences between them are caused by accident only. In other words, the efficacies of both drugs **A** and **B** are the same.\n",
    "\n",
    "\n",
    "### Table 1\n",
    "\n",
    "-| Improvement YES |\tImprovement NO \t | \n",
    ":---:|:---------:|:------------:|:----:\n",
    "Drug $A$  | a | b | a+b \n",
    "Drug $B$  | c | d | c+d \n",
    "-------------------|-------------|-----------------|-------\n",
    "- | a+c | b+d | N \n",
    "\n",
    "### Task 1: \n",
    "Derive the formula for computing the probability that the table with results will have the same values as in Table 1 for given values $a, b, c$ and $d$ $(N=a+b+c+d)$ assuming that the null hypothesis is true. \n",
    "\n",
    "Remark: Stating that the probability corresponds to the hypergeometric probability distributions is not enough. You should explain what is the meaning of all binomial coefficients or factorials in the formulas you will use!\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Task 1 Solution:\n",
    "Conditional on the margins of the table, $a$ is distributed as an hypergeometric distribution with $a+b$ draws from a population with $a+c$ successes and $b+d$ failures, as is explained on wikipedia (https://en.wikipedia.org/wiki/Fisher%27s_exact_test).\n",
    "\n",
    "The probability of obtaining such a set of values from hypergeometric distribution is:\n",
    "$$ p = \\frac{\\binom{a+c}{a} \\binom{b+d}{b}} {\\binom{N}{a+b}} = \\frac{\\binom{a+c}{c} \\binom{b+d}{d}} {\\binom{N}{c+d}} = \\frac{(a+c)! (b+d)! (a+b)! (c+d)!}{a! b! c! d! N!} $$\n",
    "\n",
    "In my own words:\n",
    "* There are $\\binom{a+c}{a}$ possible ways how the $a$ number of patients who were given the drug $A$ can be chosen from $(a+c)$ patients with improvement.\n",
    "* There are $\\binom{b+d}{b}$ possible ways how the $b$ number of patients who were given the drug $B$ can be chosen from $(b+d)$ patients with**OUT** improvement.\n",
    "* There are $\\binom{N}{a+b}$ possible ways how the $a+b$ number of patients who were given the drug $A$ can be chosen from $(N)$ set of all patients.\n",
    "\n",
    "By **x ways how the k can be chosen from n** I mean in this case how many combinations of k patients are possible given the number of patients n."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Table 2\n",
    "\n",
    "-| Improvement YES |\tImprovement NO \t | \n",
    ":---:|:---------:|:------------:|:----:\n",
    "Drug $A$  | 8 | 1 | 9\n",
    "Drug $B$  | 4 | 5 | 9 \n",
    "-------------------|-------------|-----------------|-------\n",
    "- | 12 | 6 | 18 \n",
    "\n",
    "### Task 2:\n",
    "Implement function $TabProb(a,b,c,d)$ that computes the probability of Table 1 assuming the null hypothesis is true using the formula you have derived in Task 1. Using the function, compute the probability of Table 2."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def TabProb(a, b, c, d):\n",
    "    numerator = np.math.factorial(a+c) * np.math.factorial(b+d) * np.math.factorial(a+b) * np.math.factorial(c+d)\n",
    "    denominator = np.math.factorial(a) * np.math.factorial(b) * np.math.factorial(c) * np.math.factorial(d) * np.math.factorial(a+b+c+d)\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.06108597285067873\n"
     ]
    }
   ],
   "source": [
    "table = np.array([[8,1], [4,5]])\n",
    "a = 8\n",
    "b = 1\n",
    "c = 4\n",
    "d = 5\n",
    "p = TabProb(a, b, c, d)\n",
    "print(p)"
   ]
  },
  {
   "source": [
    "The difference between the drugs A and B is evident. Is this difference statistically significant? That is, assuming that both samples A and B are from the same probability distribution, what is the probability that two samples differ to the same or even higher extent? If this probability is small, e.g., at most α=0.05\n",
    ", we can state with the high confidence (1−α)=0.95 that the null hypothesis is not valid. Based on the marginal sums (a+b, c+d, a+c and b+d), we can easily compute that the expected value of the field $a$ is 6. The notion \"differing to the same or even higher extent\" can be understood in two ways\n",
    "\n",
    "1. one-sided - only the values of a that are on one side from the expected value; in our case, the values 8 and 9, or\n",
    "2. two-sided - all the values of a\n",
    "such that |a−6|≥8−6; in our case, the values 0, 1, 2, 3, 4, 8 and 9.\n",
    "\n",
    "In case 1, we use a one-sided test, in case 2, we use a two-sided test.\n",
    "\n",
    "### Is this difference statistically significant? \n",
    "Probability that two samples differ to the same or even higher extent is approximately 0.061 which is larger than α=0.05. The difference between the drugs A and B is therefore **NOT** statistically significant. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Task 3:\n",
    "\n",
    "* Answer the following question\n",
    "    * In general, which of the four combinations of tests \n",
    "    \n",
    "    {one-sided, two-sided}×{Fisher's test,χ2-test} are meaningful?\n",
    "\n",
    "* While ignoring the requirement that χ2-test can be used only if all counts in the contingency table are at least 5, perform and evaluate all meaningful combinations of the above tests at the significance level α=0.05. Compare the results of the tests. For computing the test use suitable functions from Python scipy library and scipy.stats.chi2.cdf(), scipy.stats.chi2.sf()and scipy.stats.chi2.isf(). Of course, scipy contains functions for computing Fisher's exact test and χ2-test. Compare your results with the results obtained by using the functions scipy.stats.fisher_exact() and scipy.stats.chi2_contingency()."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Answers Task 3:\n",
    "1. χ2-test is a two-sided test which uses one-sided critical region. \n",
    "The χ2-test statistic is:\n",
    "$$ X^2 = \\sum_{i,j} \\frac{(o_{ij}-e_{ij})^2}{e_{ij}}$$\n",
    "Where $o_{ij}$ are the observed counts in cell $[i,j]$ and $e_{ij}$ are the expected cell count in cell $[i,j]$. As the numerator of each term is **squared** difference between observation and expected value. So it makes no difference when $o_{ij} < e_{ij}$ or $o_{ij} > e_{ij}$. Therefore χ2-test is always two-sided test even if the critical region is defined in one (the right) tail of the χ2 distribution.\n",
    "(My main source: https://stats.stackexchange.com/questions/171074/chi-square-test-why-is-the-chi-squared-test-a-one-tailed-test/171084#171084)\n",
    "Other combinations of test are meaningful.\n",
    "\n",
    "2. perform and evaluate all meaningful combinations:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "table:\n [[8 1]\n [4 5]]\nexpected table:\n [[6. 3.]\n [6. 3.]]\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "print(\"table:\\n\", table)\n",
    "row_sum = np.sum(table, axis=1)\n",
    "#print(row_sum)\n",
    "column_sum = np.sum(table, axis=0)\n",
    "#print(column_sum)\n",
    "n = np.sum(table)\n",
    "#print(n)\n",
    "\n",
    "expected_table = np.zeros((2, 2))\n",
    "expected_table = np.outer(row_sum,column_sum) / n\n",
    "\n",
    "print(\"expected table:\\n\", expected_table)"
   ]
  },
  {
   "source": [
    "Fisher's exact tests:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fisher two-sided: 0.13122171945701358\ncontrol: (10.0, 0.13122171945701377)\nFisher one-sided: 0.06561085972850679\ncontrol: (10.0, 0.06561085972850689)\n"
     ]
    }
   ],
   "source": [
    "# two-sided\n",
    "# a=1 and a=2 impossible\n",
    "p_a3 = TabProb(3,6,9,0) # a=3\n",
    "p_a4 = TabProb(4,5,8,1) # a=4\n",
    "\n",
    "p_a8 = TabProb(8,1,4,5) # a=3\n",
    "p_a9 = TabProb(9,0,3,6) # a=4\n",
    "\n",
    "p2 = p_a3 + p_a4 + p_a8 + p_a9\n",
    "print(\"Fisher two-sided:\",p2)\n",
    "print(\"control:\", scipy.stats.fisher_exact(table, alternative=\"two-sided\"))\n",
    "\n",
    "# one-sided\n",
    "\n",
    "p1 = p_a8 + p_a9 \n",
    "print(\"Fisher one-sided:\", p1)\n",
    "print(\"control:\", scipy.stats.fisher_exact(table, alternative=\"greater\"))"
   ]
  },
  {
   "source": [
    "χ2-test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Chi squared p-value using sf func.:  0.04550026389635857\nChi squared p-value using cdf func.: 0.04550026389635853\nControl p-value without correction:  0.04550026389635857\nControl p-value with correction:     0.13361440253771584\n"
     ]
    }
   ],
   "source": [
    "test = np.sum(np.square(table - expected_table) / expected_table)\n",
    "p_sf = scipy.stats.chi2.sf(test, df=1)\n",
    "p_cdf = 1 - scipy.stats.chi2.cdf(test, df=1)\n",
    "print(\"Chi squared p-value using sf func.: \", p_sf)\n",
    "print(\"Chi squared p-value using cdf func.:\", p_cdf)\n",
    "\n",
    "chi2, p, dof, expected = scipy.stats.chi2_contingency(table, correction=False)\n",
    "print(\"Control p-value without correction: \", p)\n",
    "chi2, p, dof, expected = scipy.stats.chi2_contingency(table, correction=True)\n",
    "print(\"Control p-value with correction:    \", p)\n"
   ]
  }
 ]
}